{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "#model.config.output_attentions = True  # 返回注意力权重\n",
    "model.config.output_hidden_states = True  # 返回隐藏状态（可选）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text,text], return_tensors=\"pt\").to(model.device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model(**model_inputs, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, seq_len, model_dim = generated_ids['hidden_states'][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tgt = generated_ids['hidden_states'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 参数\n",
    "d_model = 896  # 隐藏维度\n",
    "nhead = 8      # 注意力头数\n",
    "num_encoder_layers = 6  # 编码器层数\n",
    "num_decoder_layers = 6  # 解码器层数\n",
    "dim_feedforward = 2048  # 前馈层中间维度\n",
    "dropout = 0.1\n",
    "\n",
    "# 定义编码器和解码器层\n",
    "encoder_layer = nn.TransformerEncoderLayer(\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    dropout=dropout,\n",
    "    batch_first=True\n",
    ")\n",
    "decoder_layer = nn.TransformerDecoderLayer(\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    dropout=dropout,\n",
    "    batch_first=True,\n",
    "    dtype=torch.bfloat16,\n",
    "    device='cuda:0'\n",
    ")\n",
    "\n",
    "# 堆叠多层\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "\n",
    "# # 输入数据\n",
    "# batch_size = 2\n",
    "# src_seq_len = 10  # 源序列长度\n",
    "# tgt_seq_len = 8   # 目标序列长度\n",
    "# src = torch.randn(batch_size, src_seq_len, d_model)  # 源序列\n",
    "# tgt = torch.randn(batch_size, tgt_seq_len, d_model)  # 目标序列\n",
    "\n",
    "# 生成掩码（自回归掩码）\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = torch.triu(torch.ones(sz, sz), diagonal=1).bool()\n",
    "    return mask\n",
    "\n",
    "# tgt_mask = generate_square_subsequent_mask(tgt_seq_len).to(src.device)\n",
    "\n",
    "# # 前向传播\n",
    "# # (1) 编码器处理源序列\n",
    "# memory = transformer_encoder(src)  # 输出形状: (batch_size, src_seq_len, d_model)\n",
    "\n",
    "# # (2) 解码器处理目标序列，并使用编码器输出进行交叉注意力\n",
    "# output = transformer_decoder(\n",
    "#     tgt,              # 目标序列\n",
    "#     memory,           # 编码器输出\n",
    "#     tgt_mask=tgt_mask # 自回归掩码\n",
    "# )  # 输出形状: (batch_size, tgt_seq_len, d_model)\n",
    "\n",
    "# # 打印结果\n",
    "# print(\"Encoder output (memory) shape:\", memory.shape)\n",
    "# print(\"Decoder output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x TransformerDecoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=896, out_features=896, bias=True)\n",
       "      )\n",
       "      (multihead_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=896, out_features=896, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=896, out_features=2048, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=2048, out_features=896, bias=True)\n",
       "      (norm1): LayerNorm((896,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((896,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((896,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      (dropout3): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_mask = generate_square_subsequent_mask(seq_len).to('cuda')\n",
    "transformer_decoder.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  ...,  True,  True,  True],\n",
       "        [False, False,  True,  ...,  True,  True,  True],\n",
       "        [False, False, False,  ...,  True,  True,  True],\n",
       "        ...,\n",
       "        [False, False, False,  ..., False,  True,  True],\n",
       "        [False, False, False,  ..., False, False,  True],\n",
       "        [False, False, False,  ..., False, False, False]], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2734,  1.8594, -0.5938,  ..., -0.0762,  0.1768,  2.0000],\n",
       "         [ 0.0491,  0.7031, -0.2598,  ..., -1.4766,  1.6094,  0.8711],\n",
       "         [-0.6484,  0.5938,  0.0854,  ..., -0.6445,  0.4648,  1.4219],\n",
       "         ...,\n",
       "         [-1.4766,  2.0781,  0.1348,  ..., -0.2891,  0.5586,  0.8047],\n",
       "         [ 0.1768,  1.5547, -0.9102,  ..., -0.7812, -0.1670, -0.1914],\n",
       "         [-0.4805,  2.4062,  0.1206,  ..., -1.1562,  0.6680,  1.8203]],\n",
       "\n",
       "        [[-1.0391,  1.1094,  0.3145,  ..., -0.4492,  0.0684,  2.0312],\n",
       "         [-0.4863,  1.5234, -0.8906,  ..., -0.2773, -0.1191, -0.1250],\n",
       "         [ 0.2246,  1.0781,  0.1523,  ..., -0.7383, -0.1602,  0.6758],\n",
       "         ...,\n",
       "         [-0.0195,  0.9961,  0.4316,  ..., -0.6016,  1.3203,  0.0354],\n",
       "         [-0.5078,  1.4219,  0.8203,  ..., -0.9883,  0.0957,  2.4062],\n",
       "         [-0.7969,  2.0938,  0.6797,  ..., -0.3145,  0.6211,  2.7188]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_decoder(tgt, tgt, tgt_mask=tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m last_hidden_states = generated_ids.hidden_states[-\u001b[32m1\u001b[39m]  \u001b[38;5;66;03m# 最后一层的隐藏状态\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 输出形状\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLast hidden states shape:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mlast_hidden_states\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLast hidden states:\u001b[39m\u001b[33m\"\u001b[39m, last_hidden_states)\n",
      "\u001b[31mAttributeError\u001b[39m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "last_hidden_states = generated_ids.hidden_states[-1]  # 最后一层的隐藏状态\n",
    "\n",
    "# 输出形状\n",
    "print(\"Last hidden states shape:\", last_hidden_states.shape)\n",
    "print(\"Last hidden states:\", last_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
      "user\n",
      "Give me a short introduction to large language model.\n",
      "assistant\n",
      "Large language models (LLMs) are artificial intelligence systems that use natural language processing techniques to understand and generate human-like text. These models can be trained on vast amounts of data, including books, articles, websites, and other forms of information, and have been used in a wide range of applications, from language translation and summarization to chatbots and virtual assistants.\n",
      "\n",
      "LLMs are typically composed of multiple layers of neural networks, which allow them to learn complex patterns and relationships within the data they are trained on. This enables them to perform tasks such as question answering, sentiment analysis, and language generation with high accuracy and speed.\n",
      "\n",
      "One of the most notable features of LLMs is their ability to process and understand a broad range of topics and contexts, making them useful for a variety of industries and applications beyond just language translation or summarization. However, like any technology, LLMs also face challenges and limitations, such as issues related to bias and transparency, which need to be addressed in order to ensure their widespread adoption and effectiveness.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = tokenizer.batch_decode(generated_ids['sequences'], skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(generated_ids['sequences'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 247, 64])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_ids['past_key_values'][1][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
