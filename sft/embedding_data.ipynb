{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c260dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA_VISIBLE_DEVICES set to: 1\n"
     ]
    }
   ],
   "source": [
    "# Requires transformers>=4.51.0\n",
    "# gpu visiableble setting\n",
    "import os\n",
    "import sys\n",
    "def set_visible_gpus(gpu_ids):\n",
    "    \"\"\"\n",
    "    Set the visible GPUs for the current process.\n",
    "    \n",
    "    Args:\n",
    "        gpu_ids (str): Comma-separated string of GPU IDs to make visible.\n",
    "    \"\"\"\n",
    "    if not isinstance(gpu_ids, str):\n",
    "        raise ValueError(\"gpu_ids must be a string\")\n",
    "    \n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpu_ids\n",
    "    print(f\"CUDA_VISIBLE_DEVICES set to: {gpu_ids}\")\n",
    "\n",
    "set_visible_gpus(\"1\")\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "def last_token_pool(last_hidden_states: Tensor,\n",
    "                 attention_mask: Tensor) -> Tensor:\n",
    "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "    if left_padding:\n",
    "        return last_hidden_states[:, -1]\n",
    "    else:\n",
    "        sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "        batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-Embedding-0.6B', padding_side='left')\n",
    "model = AutoModel.from_pretrained('Qwen/Qwen3-Embedding-0.6B')\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving.\n",
    "#model = AutoModel.from_pretrained('Qwen/Qwen3-Embedding-0.6B', attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)\n",
    "model = model.to(\"cuda\")\n",
    "max_length = 8192\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d54ecb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataframe from 'mag7_aigc_results.parquet'\n",
    "import pandas as pd\n",
    "df = pd.read_parquet('mag7_aigc_results.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e697f193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of content column: 39861 at index 4180\n"
     ]
    }
   ],
   "source": [
    "#scan df and find max length of column content  and idx\n",
    "max_len = 0\n",
    "max_idx = 0\n",
    "for i in range(len(df)):\n",
    "    if len(df.iloc[i, 0]) > max_len:\n",
    "        max_len = len(df.iloc[i, 0])\n",
    "        max_idx = i\n",
    "print(f\"Max length of content column: {max_len} at index {max_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb3fe5ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39861"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.iloc[max_idx, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55abd68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json, tqdm\n",
    "\n",
    "# def get_embedding(texts, batch_size=32):\n",
    "#     \"\"\"\n",
    "#     Generate embeddings for a list of texts using batch processing to avoid OOM.\n",
    "    \n",
    "#     Args:\n",
    "#         texts (list): List of text strings to embed.\n",
    "#         batch_size (int): Number of texts to process per batch.\n",
    "    \n",
    "#     Returns:\n",
    "#         list: List of embedding vectors (as lists).\n",
    "#     \"\"\"\n",
    "#     all_embeddings = []\n",
    "    \n",
    "#     # Process texts in batches\n",
    "#     for i in tqdm.tqdm(range(0, len(texts), batch_size), desc=\"Processing batches\"):\n",
    "#         batch_texts = texts[i:i + batch_size]\n",
    "        \n",
    "#         # Tokenize the batch\n",
    "#         encoded_input = tokenizer(\n",
    "#             batch_texts,\n",
    "#             padding=True,\n",
    "#             truncation=True,\n",
    "#             max_length=max_length,\n",
    "#             return_tensors='pt'\n",
    "#         ).to(\"cuda\")\n",
    "        \n",
    "#         # Get model outputs\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**encoded_input)\n",
    "#             last_hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "#         # Apply last token pooling\n",
    "#         embeddings = last_token_pool(last_hidden_states, encoded_input['attention_mask'])\n",
    "        \n",
    "#         # Normalize embeddings\n",
    "#         embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "#         # Store embeddings\n",
    "#         all_embeddings.extend(embeddings.cpu().numpy().tolist())\n",
    "        \n",
    "#         # Clear GPU memory\n",
    "#         torch.cuda.empty_cache()\n",
    "    \n",
    "#     return all_embeddings\n",
    "\n",
    "# def process_dataframe(df, batch_size=16):\n",
    "#     \"\"\"\n",
    "#     Process the input DataFrame to generate embeddings for each column's text data.\n",
    "    \n",
    "#     Args:\n",
    "#         df (pd.DataFrame): Input DataFrame with columns containing JSON strings.\n",
    "#         batch_size (int): Number of texts to process per batch.\n",
    "    \n",
    "#     Returns:\n",
    "#         pd.DataFrame: Output DataFrame with embeddings in the same structure.\n",
    "#     \"\"\"\n",
    "#     output_df = pd.DataFrame(index=df.index, columns=df.columns)\n",
    "    \n",
    "#     for column in tqdm.tqdm(df.columns, desc=\"Processing columns\"):\n",
    "#         # Extract texts from the column\n",
    "#         texts = []\n",
    "#         for item in df[column]:\n",
    "#             try:\n",
    "#                 # Parse JSON string to dictionary and extract relevant text\n",
    "                \n",
    "#                 texts.append(item)\n",
    "#             except json.JSONDecodeError:\n",
    "#                 texts.append(\"\")\n",
    "        \n",
    "#         # Generate embeddings for the texts\n",
    "#         embeddings = get_embedding(texts, batch_size=batch_size)\n",
    "        \n",
    "#         # Store embeddings as JSON strings\n",
    "#         output_df[column] = [json.dumps(emb) for emb in embeddings]\n",
    "    \n",
    "#     return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfd88d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import json, gc\n",
    "def get_embedding(texts, batch_size=4):\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of texts using batch processing to avoid OOM.\n",
    "    \n",
    "    Args:\n",
    "        texts (list): List of text strings to embed.\n",
    "        batch_size (int): Number of texts to process per batch.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of embedding vectors (as lists).\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in tqdm.tqdm(range(0, len(texts), batch_size), desc=\"Processing batches\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        \n",
    "        # Tokenize the batch\n",
    "        encoded_input = tokenizer(\n",
    "            batch_texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors='pt'\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        # Get model outputs\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded_input)\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        # Apply last token pooling\n",
    "        embeddings = last_token_pool(last_hidden_states, encoded_input['attention_mask'])\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "        \n",
    "        # Store embeddings\n",
    "        all_embeddings.extend(embeddings.cpu().numpy().tolist())\n",
    "        \n",
    "        # Explicitly delete tensors and clear memory\n",
    "        del encoded_input, outputs, last_hidden_states, embeddings\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        # Log memory usage for debugging\n",
    "        #print(f\"Batch {i//batch_size + 1}: GPU memory allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    return all_embeddings\n",
    "\n",
    "def process_dataframe(df, batch_size=16, chunk_size=8000):\n",
    "    \"\"\"\n",
    "    Process the input DataFrame to generate embeddings for each column's text data.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame with columns containing JSON strings.\n",
    "        batch_size (int): Number of texts to process per batch.\n",
    "        chunk_size (int): Number of rows to process per chunk.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Output DataFrame with embeddings in the same structure.\n",
    "    \"\"\"\n",
    "    output_df = pd.DataFrame(index=df.index, columns=df.columns)\n",
    "    \n",
    "    for column in tqdm.tqdm(df.columns, desc=\"Processing columns\"):\n",
    "        # Process rows in chunks to reduce memory usage\n",
    "        for start_idx in range(0, len(df), chunk_size):\n",
    "            end_idx = min(start_idx + chunk_size, len(df))\n",
    "            chunk_texts = []\n",
    "            \n",
    "            # Extract texts from the chunk\n",
    "            for item in df[column].iloc[start_idx:end_idx]:\n",
    "                try:\n",
    "                    # Parse JSON string to dictionary and extract relevant text\n",
    "                    #item length limited to 2048 characters\n",
    "                    if isinstance(item, str):\n",
    "                        item = item[:2048]  # Limit to 2048 characters\n",
    "                    else:\n",
    "                        item = str(item)[:2048]  # Convert to string and limit\n",
    "                    chunk_texts.append(item)\n",
    "                    \n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Warning: JSON decode error in column {column}: {e}\")\n",
    "                    chunk_texts.append(\"\")\n",
    "            \n",
    "            # Generate embeddings for the chunk\n",
    "            embeddings = get_embedding(chunk_texts, batch_size=batch_size)\n",
    "            \n",
    "            # Store embeddings in the output DataFrame\n",
    "            output_df[column].iloc[start_idx:end_idx] = [json.dumps(emb) for emb in embeddings]\n",
    "    \n",
    "    return output_df\n",
    "\n",
    "# Example usage\n",
    "# input_df = pd.DataFrame({\n",
    "#     'AAPL': ['```python\\n{\"股票代码\": \"AAPL\", \"日期\": \"2023-10-01\"}\\n```'] * 10000,\n",
    "#     'MSFT': ['```python\\n{\"股票\": \"MSFT\", \"日期\": \"2023-10-01\"}\\n```'] * 10000,\n",
    "# })\n",
    "# output_df = process_dataframe(input_df, batch_size=4, chunk_size=1000)\n",
    "# output_df.to_csv('embedded_data.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6827e2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing columns:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 310/310 [08:36<00:00,  1.67s/it]\n",
      "/tmp/ipykernel_3111919/1863417875.py:91: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  output_df[column].iloc[start_idx:end_idx] = [json.dumps(emb) for emb in embeddings]\n",
      "Processing batches: 100%|██████████| 310/310 [07:53<00:00,  1.53s/it]\n",
      "Processing batches: 100%|██████████| 310/310 [07:58<00:00,  1.54s/it]\n",
      "Processing batches: 100%|██████████| 310/310 [07:56<00:00,  1.54s/it]\n",
      "Processing batches: 100%|██████████| 310/310 [08:03<00:00,  1.56s/it]\n",
      "Processing batches: 100%|██████████| 310/310 [07:42<00:00,  1.49s/it]\n",
      "Processing batches: 100%|██████████| 310/310 [07:59<00:00,  1.55s/it]\n",
      "Processing columns: 100%|██████████| 7/7 [56:37<00:00, 485.37s/it]\n"
     ]
    }
   ],
   "source": [
    "embedding_df = process_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05687358",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_df.to_parquet('mag7_aigc_results_embedding.parquet', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22be90eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AAPL</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>GOOGL</th>\n",
       "      <th>AMZN</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>META</th>\n",
       "      <th>TSLA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2006-03-27</th>\n",
       "      <td>[-0.010473893024027348, 0.03431328013539314, -...</td>\n",
       "      <td>[-0.03791709616780281, 0.025187354534864426, -...</td>\n",
       "      <td>[-0.0244185458868742, 0.05697000026702881, -0....</td>\n",
       "      <td>[-0.036835409700870514, 0.09445228427648544, -...</td>\n",
       "      <td>[0.03612320497632027, 0.044321343302726746, -0...</td>\n",
       "      <td>[-0.03690069913864136, 0.016980359330773354, -...</td>\n",
       "      <td>[0.02404155023396015, 0.060831159353256226, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-03-28</th>\n",
       "      <td>[0.004550086800009012, 0.05633486434817314, -0...</td>\n",
       "      <td>[-0.024092519655823708, 0.04828929901123047, -...</td>\n",
       "      <td>[-0.031112181022763252, 0.027739599347114563, ...</td>\n",
       "      <td>[-0.021372055634856224, 0.06737802922725677, -...</td>\n",
       "      <td>[-0.00650930218398571, 0.025904914364218712, -...</td>\n",
       "      <td>[-0.0016450012335553765, 0.03578471019864082, ...</td>\n",
       "      <td>[0.0465957410633564, 0.02111438475549221, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-03-29</th>\n",
       "      <td>[-0.007588344160467386, 0.056205328553915024, ...</td>\n",
       "      <td>[-0.03325615078210831, 0.02914321795105934, -0...</td>\n",
       "      <td>[-0.015981199219822884, 0.024827459827065468, ...</td>\n",
       "      <td>[-0.026570795103907585, 0.07063396275043488, -...</td>\n",
       "      <td>[0.013584701344370842, 0.034102290868759155, -...</td>\n",
       "      <td>[-0.039836686104536057, 0.016013996675610542, ...</td>\n",
       "      <td>[0.03377041220664978, 0.06259383261203766, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-03-30</th>\n",
       "      <td>[-0.024958888068795204, 0.06636907905340195, -...</td>\n",
       "      <td>[-0.042656801640987396, 0.010825795121490955, ...</td>\n",
       "      <td>[-0.038396649062633514, 0.03181783854961395, -...</td>\n",
       "      <td>[-0.01843183860182762, 0.04819761961698532, -0...</td>\n",
       "      <td>[0.03162645176053047, 0.03200918436050415, -0....</td>\n",
       "      <td>[-0.017583539709448814, 0.0014630352379754186,...</td>\n",
       "      <td>[0.048341259360313416, 0.016582613810896873, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006-03-31</th>\n",
       "      <td>[-0.003448369214311242, 0.0480380542576313, -0...</td>\n",
       "      <td>[-0.026733791455626488, 0.024036569520831108, ...</td>\n",
       "      <td>[-0.032884642481803894, 0.03423741087317467, -...</td>\n",
       "      <td>[-0.04097476601600647, 0.04234614968299866, -0...</td>\n",
       "      <td>[0.006437849253416061, 0.03503504768013954, -0...</td>\n",
       "      <td>[-0.018855085596442223, 0.018116049468517303, ...</td>\n",
       "      <td>[0.014296467415988445, 0.06189926713705063, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         AAPL  \\\n",
       "2006-03-27  [-0.010473893024027348, 0.03431328013539314, -...   \n",
       "2006-03-28  [0.004550086800009012, 0.05633486434817314, -0...   \n",
       "2006-03-29  [-0.007588344160467386, 0.056205328553915024, ...   \n",
       "2006-03-30  [-0.024958888068795204, 0.06636907905340195, -...   \n",
       "2006-03-31  [-0.003448369214311242, 0.0480380542576313, -0...   \n",
       "\n",
       "                                                         MSFT  \\\n",
       "2006-03-27  [-0.03791709616780281, 0.025187354534864426, -...   \n",
       "2006-03-28  [-0.024092519655823708, 0.04828929901123047, -...   \n",
       "2006-03-29  [-0.03325615078210831, 0.02914321795105934, -0...   \n",
       "2006-03-30  [-0.042656801640987396, 0.010825795121490955, ...   \n",
       "2006-03-31  [-0.026733791455626488, 0.024036569520831108, ...   \n",
       "\n",
       "                                                        GOOGL  \\\n",
       "2006-03-27  [-0.0244185458868742, 0.05697000026702881, -0....   \n",
       "2006-03-28  [-0.031112181022763252, 0.027739599347114563, ...   \n",
       "2006-03-29  [-0.015981199219822884, 0.024827459827065468, ...   \n",
       "2006-03-30  [-0.038396649062633514, 0.03181783854961395, -...   \n",
       "2006-03-31  [-0.032884642481803894, 0.03423741087317467, -...   \n",
       "\n",
       "                                                         AMZN  \\\n",
       "2006-03-27  [-0.036835409700870514, 0.09445228427648544, -...   \n",
       "2006-03-28  [-0.021372055634856224, 0.06737802922725677, -...   \n",
       "2006-03-29  [-0.026570795103907585, 0.07063396275043488, -...   \n",
       "2006-03-30  [-0.01843183860182762, 0.04819761961698532, -0...   \n",
       "2006-03-31  [-0.04097476601600647, 0.04234614968299866, -0...   \n",
       "\n",
       "                                                         NVDA  \\\n",
       "2006-03-27  [0.03612320497632027, 0.044321343302726746, -0...   \n",
       "2006-03-28  [-0.00650930218398571, 0.025904914364218712, -...   \n",
       "2006-03-29  [0.013584701344370842, 0.034102290868759155, -...   \n",
       "2006-03-30  [0.03162645176053047, 0.03200918436050415, -0....   \n",
       "2006-03-31  [0.006437849253416061, 0.03503504768013954, -0...   \n",
       "\n",
       "                                                         META  \\\n",
       "2006-03-27  [-0.03690069913864136, 0.016980359330773354, -...   \n",
       "2006-03-28  [-0.0016450012335553765, 0.03578471019864082, ...   \n",
       "2006-03-29  [-0.039836686104536057, 0.016013996675610542, ...   \n",
       "2006-03-30  [-0.017583539709448814, 0.0014630352379754186,...   \n",
       "2006-03-31  [-0.018855085596442223, 0.018116049468517303, ...   \n",
       "\n",
       "                                                         TSLA  \n",
       "2006-03-27  [0.02404155023396015, 0.060831159353256226, -0...  \n",
       "2006-03-28  [0.0465957410633564, 0.02111438475549221, -0.0...  \n",
       "2006-03-29  [0.03377041220664978, 0.06259383261203766, -0....  \n",
       "2006-03-30  [0.048341259360313416, 0.016582613810896873, -...  \n",
       "2006-03-31  [0.014296467415988445, 0.06189926713705063, -0...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2fa5e538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4958, 7168)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "# Assuming A is your DataFrame with shape [100, 7], where each cell is an array of length 1024\n",
    "# Example placeholder for A (replace with your actual DataFrame)\n",
    "# A = pd.DataFrame(...)\n",
    "\n",
    "# Reshape the DataFrame\n",
    "def reshape_dataframe(df):\n",
    "    # Get the number of rows and columns\n",
    "    num_rows, num_cols = df.shape\n",
    "    array_length = 1024  # Length of each array in a cell\n",
    "    \n",
    "    # Initialize an empty array to store the reshaped data\n",
    "    reshaped_data = np.zeros((num_rows, num_cols * array_length))\n",
    "\n",
    "    # Iterate over each row\n",
    "    for i in range(num_rows):\n",
    "        # Concatenate all arrays in the row\n",
    "        # df.iloc[i, j]的内容是一个长度为1024的数组,使用字符串的形式，需要先转型成数组\n",
    "        #df.iloc[i, j] = np.array(df.iloc[i, j])\n",
    "        row_data = np.concatenate([ast.literal_eval(df.iloc[i, j]) for j in range(num_cols)])\n",
    "        reshaped_data[i, :] = row_data\n",
    "    \n",
    "    # Create a new DataFrame with the reshaped data\n",
    "    reshaped_df = pd.DataFrame(reshaped_data)\n",
    "    reshaped_df.index = df.index  # Preserve the original index\n",
    "    return reshaped_df\n",
    "\n",
    "# Apply the reshaping function\n",
    "reshaped_A = reshape_dataframe(embedding_df)\n",
    "\n",
    "# Verify the shape\n",
    "print(reshaped_A.shape)  # Should output (100, 7168) since 7 * 1024 = 7168"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "49b66c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.010473893024027348"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshaped_A.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd88a327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming DataFrame A is already loaded with shape [100, 7], where each cell is a numpy array of length 1024\n",
    "# For demonstration, let's create a sample A if not provided\n",
    "tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'META', 'TSLA']\n",
    "\n",
    "# Step 2: Create MultiIndex for columns\n",
    "features = list(range(1024))\n",
    "multi_index = pd.MultiIndex.from_product([tickers, features], names=['tickers', 'features'])\n",
    "\n",
    "# Step 3: Create DataFrame B\n",
    "reshaped_A.columns = multi_index\n",
    "\n",
    "# Output the resulting DataFrame B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f595a0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_A.to_parquet('mag7_aigc_results_embedding_reshaped.parquet', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509301e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46543c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "重塑过程中出错：无法将 [0, 0] 处的单元格转换为 NumPy 数组：could not convert string to float: '[-0.010473893024027348, 0.03431328013539314, -0.0013378848088905215, 0.023608606308698654, 0.062483858317136765, 0.009751654230058193, 0.02134564518928528, 0.025454487651586533, -0.05338792875409126, -0.049225810915231705, 0.04630224034190178, -0.028764022514224052, 0.033088065683841705, 0.002100762678310275, -0.03657372668385506, -0.00015793122292961925, -0.06244644150137901, -0.031432051211595535, -0.06327351182699203, 0.027225056663155556, 0.07172472774982452, -0.016771454364061356, -0.03620494157075882, 0.027216436341404915, -0.04180443286895752, -0.09769964963197708, 0.03399081528186798, 0.003334772540256381, -0.0521385483443737, 0.010352633893489838, 0.03474702686071396, -0.008120913058519363, -0.03469813987612724, -0.0065373810939490795, 0.00750644039362669, -0.0010695707751438022, 0.005285385996103287, 0.03388827294111252, -0.01708252914249897, 0.021881017833948135, -0.004998895805329084, 0.01859688013792038, -0.017793938517570496, 0.02571718953549862, -0.012646599672734737, -0.006117354612797499, 0.005662370473146439, -0.009718365967273712, 0.03468162566423416, 0.03601006418466568, 0.014486434869468212, 0.029716936871409416, 0.020279482007026672, -0.002541824011132121, -0.004526879172772169, 0.033476248383522034, 0.01222110353410244, 0.03437791019678116, 0.03872324898838997, 0.03677693381905556, -0.03081602044403553, 0.004138288553804159, 0.014371614903211594, -0.03289750590920448, -0.013029471039772034, -0.028780125081539154, 0.0014196948613971472, 0.013726580888032913, 0.00961828138679266, -0.0090170381590724, 0.028967659920454025, 0.029734261333942413, -0.039060842245817184, -0.0015539322048425674, 0.06157427281141281, 0.06139473617076874, 0.07219503819942474, 0.033840883523225784, 0.04711959883570671, 0.05145951360464096, 0.014393333345651627, 0.12566371262073517, 0.0261522326618433, -0.004930369555950165, 0.0017426538979634643, 0.03170698881149292, -0.020208517089486122, -0.04709481820464134, 0.030088745057582855, 0.026359084993600845, -0.03884251415729523, -0.02653665468096733, -0.053431056439876556, 0.007093572989106178, 0.025456346571445465, 0.02941562607884407, 0.03928995877504349, 0.010801034048199654, -0.03401615098118782, 0.004012101795524359, -0.023051220923662186, -0.0011380752548575401, -0.052852001041173935, 0.027752725407481194, -0.058478306978940964, 0.029510535299777985, -0.03707737475633621, 0.0422196127474308, -0.023608990013599396, -0.004936323966830969, 0.03197786957025528, -0.05814361572265625, -0.011442714370787144, 0.020024651661515236, 0.03491010144352913, 0.027338795363903046, -0.00048476349911652505, 0.04568342864513397, 0.001086403033696115, -0.025775767862796783, -0.003136965911835432, 0.05243885889649391, 0.005529691930860281, -0.00042833350016735494, 0.05865760147571564, 0.009250265546143055, -0.02579738385975361, -0.006898720748722553, -0.01233287900686264, 0.0027493333909660578, -0.008850044570863247, -0.04688649624586105, -0.02704884484410286, 0.011922290548682213, -0.01398794911801815, -0.02859369106590748, 0.020893283188343048, -0.023483680561184883, -0.009446762502193451, 0.01405629888176918, -0.016451211646199226, -0.019892970100045204, 0.03240450099110603, -0.0040729232132434845, -0.007095709443092346, 0.004078038036823273, -0.011468064039945602, 0.028145968914031982, 0.010105055756866932, -0.001750439521856606, 0.03213975951075554, 0.009681317955255508, 0.0012462971499189734, -0.0006837694672867656, -0.010209658183157444, 0.014267049729824066, -0.002292637713253498, -0.033345188945531845, 0.019406452775001526, -0.015418016351759434, -0.011387033388018608, 0.0036321752704679966, 0.03230048716068268, 0.0022763069719076157, -0.03317421302199364, 0.023843461647629738, 0.005216449033468962, 0.002221255097538233, 0.02110048569738865, 0.02467881329357624, -0.004929039627313614, 0.02626992203295231, 0.014071457087993622, 0.0561404787003994, -0.012909669429063797, -0.006776188965886831, -0.0026067658327519894, -0.023198071867227554, -0.002917305566370487, -0.010112032294273376, -0.00582464225590229, -0.0355633944272995, 0.022491345182061195, 0.013065602630376816, 0.03285405784845352, -0.014842822216451168, 0.039213571697473526, -0.00807261187583208, 0.03555203601717949, -0.06046655774116516, 0.0031677973456680775, 0.02511787600815296, 0.0671258419752121, 0.017165865749120712, -0.015294128097593784, -0.012307348661124706, -0.06279134005308151, -0.013322943821549416, 0.022344203665852547, -0.0006073949043639004, 0.0010071261785924435, 0.020236456766724586, 0.002694553229957819, 0.04503099247813225, 0.009480336681008339, -0.038535673171281815, 0.013877150602638721, 0.010014602914452553, 0.03873029351234436, -0.01895584538578987, -0.004304832313209772, -0.027748199179768562, -0.06369075924158096, -0.05638086050748825, -0.032207585871219635, 0.021569890901446342, 0.05800005421042442, 0.011850297451019287, -0.022314267233014107, 0.0034981982316821814, 0.0007705289754085243, 0.001590172410942614, -0.039331670850515366, 0.002897814614698291, 0.07809251546859741, -0.04704240337014198, 0.007125404663383961, 0.03048267960548401, -0.012746886350214481, 0.007650644052773714, 0.023932166397571564, 0.01820334419608116, 0.005414638668298721, -0.006784792989492416, 0.00515883369371295, -0.02615208365023136, -0.010349458083510399, -0.027791570872068405, 0.01715470850467682, 0.01850125379860401, -0.02384241484105587, -0.0008161016157828271, -0.01903071627020836, -0.00428437814116478, -0.008863955736160278, 0.01567685976624489, -0.004592106677591801, -0.04151519015431404, -0.01851961575448513, -0.07255125045776367, 0.04571319371461868, -0.07807250320911407, 0.019146066159009933, -0.03573305904865265, -0.0024979040026664734, 0.015776298940181732, 0.044545382261276245, -0.014084811322391033, 0.0022297545801848173, -0.005965925753116608, -0.008528802543878555, -0.008357470855116844, -0.015529298223555088, 0.009178447537124157, -0.02988451160490513, -0.004183405544608831, -0.017727626487612724, -0.017974432557821274, 0.012252715416252613, -0.00262508075684309, -0.02950577437877655, 0.023020612075924873, 0.07175789028406143, 0.004103686194866896, -0.0058144330978393555, 0.011992829851806164, 0.009105994366109371, -0.06324993818998337, -0.04499374330043793, 0.07719871401786804, -0.044348832219839096, -0.026364754885435104, -0.01875055395066738, -0.004472765140235424, 0.0704789087176323, 0.016772260889410973, 0.0108557203784585, -0.043400879949331284, -0.037252649664878845, -0.06692416220903397, -0.0019335756078362465, -0.006169783882796764, -0.0007644316065125167, 0.0016126857372000813, 0.008396215736865997, 0.06052463874220848, -0.022855374962091446, 0.023193152621388435, -0.023973386734724045, -0.009741582907736301, 0.010579964146018028, 0.0053151012398302555, -0.0008990130736492574, -0.03385916352272034, -0.05487005040049553, -0.016206840053200722, 0.04879869893193245, 0.06800450384616852, 0.04350944608449936, 0.018882235512137413, -0.003872930072247982, -0.02173534594476223, 0.05184241011738777, 0.010016749612987041, 0.010945369489490986, -0.02833324484527111, 0.06790996342897415, -0.01757333241403103, 0.018002524971961975, 0.018920129165053368, -0.03313513845205307, 0.03906412795186043, -0.024481341242790222, -0.0036814443301409483, 0.020814988762140274, 0.026151709258556366, -0.015399159863591194, 0.00938759371638298, 0.0015953691909089684, -0.015624895691871643, 0.009040902368724346, 0.011167604476213455, 0.03883825242519379, 0.0073959361761808395, -0.020488586276769638, 0.019503580406308174, -0.07107783854007721, 0.013134727254509926, 0.04240505397319794, -0.03038839064538479, 0.0034343302249908447, 0.057076387107372284, 0.0204162634909153, -0.09595329314470291, -0.0017235560808330774, -0.005214174743741751, -0.06655590236186981, 0.055816177278757095, -0.0040112873539328575, -0.0030372003093361855, 0.0073607605881989, 0.016689414158463478, 0.040020231157541275, 0.023513292893767357, 0.005771842319518328, -0.03775196149945259, -0.05672433227300644, 0.04588555172085762, -0.012129315175116062, 0.0014711179537698627, -0.010352028533816338, -0.009550081565976143, 0.023716261610388756, 0.01802675798535347, -0.033254075795412064, 0.09693626314401627, -0.043938055634498596, -0.0338599868118763, -0.013084794394671917, -0.02966218814253807, 0.028912506997585297, -0.04322167485952377, 0.031136158853769302, 0.0348338708281517, -0.02833654172718525, 0.008294835686683655, 0.016912490129470825, -0.09265188127756119, -0.009801508858799934, 0.02705954760313034, -0.029428336769342422, 0.02734888158738613, -0.010388853028416634, 0.017023155465722084, -0.008383399806916714, -0.011176503263413906, -0.046809498220682144, -0.045005425810813904, -0.02977909706532955, 0.008603105321526527, 0.019998444244265556, 0.042261503636837006, 0.0005845011328347027, 0.05003424361348152, -0.019132565706968307, -0.002416935283690691, -0.0708417221903801, -0.03015962429344654, 0.027459777891635895, -0.03756452351808548, 0.06949254870414734, -0.029482347890734673, -0.011993122287094593, 0.03619278594851494, -0.029933767393231392, 0.02506893500685692, -0.0146692069247365, -0.028246117755770683, -0.004174639470875263, -0.01064119953662157, -0.022360552102327347, -0.029631689190864563, -0.015732750296592712, -0.026713309809565544, 0.01617768406867981, -0.005862424150109291, 0.06239710748195648, 0.014108083210885525, -0.011707418598234653, -0.03238169103860855, -0.008986576460301876, 0.07619446516036987, -0.030726008117198944, -0.041783660650253296, 0.04476100206375122, 0.058374471962451935, -0.014882490038871765, 0.026596393436193466, -0.0005264437641017139, -0.029031341895461082, -0.03619116544723511, 0.05375668779015541, -0.036432210355997086, 0.00466539291664958, -0.00469293212518096, 0.0408158004283905, 0.032715365290641785, 0.01856786012649536, -0.022990992292761803, -0.017222942784428596, 0.021277325227856636, -0.03309737518429756, -0.07573016732931137, -0.014844021759927273, -0.02543904446065426, 0.0211460143327713, -0.016023050993680954, 0.05013102665543556, -0.031080124899744987, 0.0004922007210552692, -0.018948540091514587, -0.004933659918606281, -0.017490191385149956, 0.0232771597802639, 0.028854304924607277, -0.04021727666258812, 0.01362235751003027, -0.050131771713495255, 0.04286954924464226, 0.03145153075456619, -0.004334199242293835, 0.039850421249866486, -0.0008912296034395695, -0.06040412560105324, 0.01680762879550457, 0.024451255798339844, 0.029396403580904007, 0.008794707246124744, 0.029500655829906464, 0.005924897734075785, 0.01881609857082367, -0.013992025516927242, -0.021424658596515656, -0.021298017352819443, 0.022671576589345932, 0.008754045702517033, 0.04729381203651428, -0.011417443864047527, -0.04148716852068901, -0.04179578274488449, -0.010192550718784332, 0.035054054111242294, -0.05442649498581886, -0.023835131898522377, 0.05398345738649368, 0.044287316501140594, 0.045782651752233505, 0.007353288121521473, -0.02696860581636429, -0.001536474796012044, 0.03152453154325485, 0.04737919941544533, -0.015161596238613129, -0.03299620747566223, 0.008102734573185444, 0.058928005397319794, -0.0005665219505317509, -0.027048103511333466, -0.03116248920559883, 0.0007473612786270678, -0.01737891137599945, 0.017536424100399017, -0.01282661221921444, 0.026214325800538063, 0.012652892619371414, -0.05012935400009155, 0.03387642279267311, -0.04491852968931198, 0.05389894172549248, 0.037839025259017944, -0.04980665445327759, -0.012154243886470795, -0.01684381440281868, 0.0031323255971074104, 0.00811594445258379, -0.001896542846225202, 0.003922180272638798, 0.03400793671607971, -0.0765911117196083, 0.017679210752248764, -0.02392684668302536, 0.02889195829629898, 0.028859857469797134, 0.005989755969494581, 0.0040515875443816185, 0.018380414694547653, -0.0002058407262666151, -0.03601911664009094, -0.039508964866399765, 0.0010962625965476036, 0.018299685791134834, -0.01590411178767681, 0.009900237433612347, 0.019033271819353104, -0.01889142394065857, -0.020741574466228485, 0.02899242378771305, -0.02845640480518341, -0.012984669767320156, 0.015532386489212513, 0.03528706356883049, 0.023161156103014946, -0.015104622580111027, -0.030199944972991943, 0.040547605603933334, -0.031059684231877327, 0.010471319779753685, 0.0006862564478069544, 0.03618306666612625, -0.008456208743155003, -0.05251818895339966, -0.020934170112013817, 0.04209122806787491, -0.0023480297531932592, -0.04403771460056305, 0.0051268464885652065, -0.03635215386748314, 0.0073127117939293385, -0.04780207946896553, -0.02147076465189457, -0.04467563331127167, 0.03413798660039902, 0.008235166780650616, 0.06712724268436432, -0.013594858348369598, -0.007468561176210642, 0.007251623552292585, -0.0373087115585804, 0.03428099304437637, -0.0030151682440191507, -0.040175437927246094, 0.017549116164445877, 0.02476508542895317, 0.036753397434949875, 0.03351355344057083, -0.001893125707283616, -0.01756240241229534, -0.007807473186403513, -0.014974396675825119, 0.015477731823921204, -0.004153247456997633, 0.031370021402835846, -0.023930445313453674, 0.013120769523084164, -0.04813806340098381, -0.0385405570268631, 0.0263981893658638, 0.00871760118752718, 0.039584092795848846, -0.03238162770867348, -0.04037025198340416, -0.019225481897592545, 0.03154490888118744, -0.044417306780815125, 0.09835868328809738, 0.0002986643521580845, -0.013543639332056046, 0.006601765286177397, 0.005287916865199804, 0.002264587441459298, 0.01559307798743248, 0.027341818436980247, -0.015476866625249386, -0.04757315292954445, -0.06260896474123001, 0.008707746863365173, -0.005381218157708645, 0.004058176651597023, -0.01895688846707344, -0.02504877559840679, 0.02343767136335373, 0.013615161180496216, -0.05058533325791359, -0.02804480493068695, -0.049982164055109024, 0.0021970667876303196, 0.014703931286931038, -0.02167917974293232, 0.03182695060968399, -0.010372614488005638, 0.03269987925887108, 0.04414606839418411, -0.006048898212611675, -0.013338563032448292, 0.0014559761621057987, 0.013709877617657185, -0.03764057904481888, -0.05806618183851242, 0.007969425059854984, -0.047423865646123886, -0.012726062908768654, -0.002621433464810252, -0.009005545638501644, -0.011537508107721806, 0.03358572721481323, 0.004737644921988249, 0.026737574487924576, -0.01912398263812065, -0.01475656870752573, -0.01601302996277809, 0.03287941962480545, -0.025854265317320824, 0.0033712235745042562, -0.04812072962522507, 0.0029314321000128984, -0.0101692583411932, 0.01309067104011774, 0.00505905132740736, 0.006154229864478111, 0.02212366834282875, -0.029175523668527603, -0.005417341366410255, 0.009506049565970898, -0.03289613500237465, -0.03587895631790161, -0.053368132561445236, -0.031192155554890633, -0.03324011340737343, -0.0013874919386580586, -0.041756030172109604, 0.028137007728219032, -0.020547455176711082, 0.018997952342033386, -0.002456923481076956, -0.05141852796077728, -0.007764397654682398, -0.014404154382646084, 0.03150453418493271, 0.01666659489274025, 0.010570052079856396, -0.04599491134285927, -0.007453902158886194, 0.03654203191399574, 0.011162618175148964, 0.03201315551996231, 0.020411590114235878, -0.010888121090829372, 0.023204999044537544, 0.003833388676866889, 0.010724212974309921, 0.00822471734136343, -0.022400423884391785, 0.008267640136182308, 0.02134700119495392, -1.8533401089371182e-05, 0.004544152412563562, 0.0006912064272910357, -0.0022119381465017796, 0.027816832065582275, 0.008549988269805908, -0.01660222001373768, -0.00019149437139276415, 0.011316932737827301, -0.061945293098688126, -0.004871075972914696, 0.05226858705282211, 0.0008257710142061114, 0.020800132304430008, -0.020321084186434746, 0.002118939533829689, -0.010225470177829266, 0.036268558353185654, 0.0071663507260382175, 0.016911739483475685, 0.011830219998955727, -0.0308303814381361, -0.04799320548772812, 0.08627600967884064, -0.05623919889330864, -0.036531832069158554, 0.02052426151931286, -0.03786331042647362, -0.03719654306769371, -0.0666048526763916, 0.02448972314596176, 0.049214351922273636, -0.012542340904474258, -0.10095023363828659, 0.009686054661870003, -0.011104023084044456, -0.020382078364491463, -0.08332239091396332, 0.0005264704232104123, -0.0034322894643992186, -0.02654976025223732, 0.019504642114043236, -0.05445561558008194, -0.06665226072072983, -0.019795289263129234, -0.004025526810437441, -0.020516986027359962, 0.026733601465821266, -0.015290511772036552, 0.014552718959748745, 0.048442497849464417, 0.0724794864654541, -0.015552808530628681, -0.0012922390596941113, -0.05363595485687256, -0.00360143487341702, -0.005818935576826334, -0.03195206820964813, -0.04132995754480362, -0.014753115363419056, 0.02890181355178356, -0.04343372955918312, 0.03624352812767029, -0.0416162945330143, 0.03641245886683464, 0.019886082038283348, 0.010739410296082497, -0.011775367893278599, -0.02144450694322586, 0.018954871222376823, 0.005818153731524944, -0.04743572697043419, -0.03152472525835037, 0.009265614673495293, 0.02054107002913952, 0.007858771830797195, 0.03676990792155266, -0.036345530301332474, -0.05833698809146881, -0.025459429249167442, 0.023731915280222893, 0.021862290799617767, -0.025685910135507584, -0.022057106718420982, -0.0186845101416111, 0.01881568320095539, -0.0026051599998027086, -0.005264595150947571, 0.03487170860171318, -0.016293156892061234, 0.023539092391729355, -0.0024830077309161425, 0.01615563966333866, -0.022214582189917564, -0.06613737344741821, -0.03540351986885071, 0.034335099160671234, 0.023347103968262672, -0.005727922543883324, -0.03550783172249794, 0.0418107695877552, 0.049725063145160675, 0.021124012768268585, 0.038711175322532654, -0.05529528111219406, -0.00478846812620759, 0.04440717026591301, -0.050872642546892166, 0.035643983632326126, 0.05843612924218178, 0.022374114021658897, 0.034448590129613876, -0.027128208428621292, -0.022450758144259453, 0.021848389878869057, -0.04399261623620987, -0.0025572585873305798, 0.028337974101305008, 0.0016800763551145792, -0.019137142226099968, -0.025037674233317375, -0.05748109519481659, -0.013613387010991573, 0.03273564577102661, 0.0040949019603431225, 0.02316034398972988, -0.013761061243712902, -0.009524166584014893, -0.009087060578167439, -0.02466011792421341, 0.02360360138118267, -0.02867727354168892, -0.06784161180257797, 0.035970818251371384, 0.0005419777589850128, 0.0036770517472177744, -0.039514169096946716, -0.02900582365691662, -0.026794372126460075, 0.06169261783361435, -0.04539160430431366, 0.03597964718937874, 0.03875201568007469, 0.0025047087110579014, -0.007226196583360434, -0.07043839246034622, -0.02992876246571541, 0.0003835855168290436, 0.023521296679973602, 0.03141394630074501, 0.055886220186948776, -0.005990244913846254, -0.009167642332613468, -0.0366218239068985, -0.030763259157538414, 0.01847865805029869, 0.0217962134629488, -0.01616978645324707, 0.026187924668192863, 0.04976272210478783, -0.010794011875987053, 0.019184760749340057, -0.022172613069415092, 0.009188159368932247, -0.0233896654099226, 0.01943841576576233, 0.011630781926214695, 0.028576981276273727, -0.020594127476215363, -0.02933293953537941, 0.03656644746661186, 0.028980176895856857, 0.0028364977333694696, 0.047278933227062225, -0.029663315042853355, -0.016078142449259758, -0.048304297029972076, -0.012733389623463154, -0.0030921082943677902, -0.00374130648560822, 0.019895408302545547, -0.005364240147173405, -0.021789416670799255, -0.07467683404684067, 0.051022958010435104, 0.021742649376392365, -0.009278199635446072, -0.011031893081963062, -0.015775393694639206, 0.011941129341721535, 0.01796417124569416, -0.009561354294419289, 0.017552338540554047, 0.003602147102355957, -0.04501334950327873, 0.04514817148447037, -0.03782375901937485, -0.009985126554965973, -0.015383001416921616, -0.016031449660658836, -0.008939915336668491, -0.0037435784470289946, -0.010811430402100086, -0.006536643486469984, -0.02640741690993309, 0.034588269889354706, 0.029039135202765465, 0.048304926604032516, -0.004827714059501886, -0.012912333011627197, -0.05912931263446808, -0.03949646279215813, 0.03666162118315697, 0.028005467727780342, 0.005618718918412924, -0.03390301764011383, 0.06460881233215332, -0.020406339317560196, 0.020150331780314445, 0.0172830019146204, 0.027066199108958244, -0.00032967020524665713, 0.008606180548667908, 0.013667749240994453, 0.02432677336037159, -0.054521165788173676, -0.05654609575867653, 0.02752767875790596, 0.07191389799118042, -0.07065173983573914, 0.04598179832100868, -0.012015054002404213, 0.016660191118717194, 0.01000196486711502, 0.025107402354478836, 0.013314735144376755, 0.01428990438580513, -0.004217430017888546, 0.019597994163632393, -0.009004084393382072, -0.023181552067399025, 0.03396577760577202, 0.058085665106773376, -0.018365614116191864, 0.04201389104127884, -0.01808338239789009, 0.0018413186771795154, 0.008426831103861332, 0.02991984784603119, -0.015742244198918343, 0.028981273993849754, -0.018139349296689034, 0.06557609885931015, -0.011219283565878868, 0.011624407023191452, 0.05615702271461487, 0.03388502821326256, 0.018348893150687218, -0.03415609523653984, 0.0034521748311817646, 0.07028229534626007, -0.004733447451144457, -0.0011708363890647888, -0.02086802013218403, 0.011062218807637691, 0.03641355410218239, -0.011520774103701115, 0.04059362784028053, -0.036956895142793655, 0.042865827679634094, 0.0011443536495789886, 0.02766442857682705, -0.017698034644126892, -0.0025242248084396124, -0.02265891805291176, 0.03701510652899742, -0.02297021634876728, -0.00593951903283596, -0.0069615403190255165, 0.03704686835408211, 0.02149140276014805, 0.0142132006585598, -0.005408677272498608, 0.030571872368454933, 0.00045585460611619055, 0.014289426617324352, 0.0034741878043860197, -0.0146173732355237, 0.016465842723846436, 0.016821827739477158, 0.007842620834708214, 0.031995341181755066, -0.017360687255859375, -0.03547641262412071, -0.03163902834057808, -0.026391785591840744, -0.013400287367403507, -0.0297516118735075, 0.05122120678424835, -0.03192302957177162, -0.006311266683042049, 0.001550849643535912, -0.04268166795372963, 0.004233947955071926, 0.03054225631058216, -0.07298646867275238, -0.023581165820360184, 0.026545491069555283, -0.016777941957116127, 0.025548076257109642, 0.034005600959062576, -0.04210088774561882, 0.0482499934732914, -0.010735377669334412, 0.013368221931159496, -0.07268375158309937, 0.01335220504552126, -0.005012414883822203, -0.01370452344417572, 0.005387318320572376, 0.018942730501294136, -0.019319433718919754, 0.018067747354507446, -0.024999888613820076, 0.008440230041742325, -0.0033461032435297966, -0.0690474659204483, 0.009784771129488945, 0.034453216940164566, -0.018725357949733734, 0.015188118442893028, -0.036055367439985275, -0.013898922130465508, -0.0024760058149695396, -0.04123585298657417, -0.04223594814538956, 0.034027762711048126, -0.015956876799464226, 0.022911418229341507, 0.041910771280527115]'\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'reshaped_A' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m tickers \u001b[38;5;241m=\u001b[39m embedding_df\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m     59\u001b[0m new_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ticker \u001b[38;5;129;01min\u001b[39;00m tickers \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1024\u001b[39m)]\n\u001b[0;32m---> 60\u001b[0m \u001b[43mreshaped_A\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m new_columns\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reshaped_A' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def reshape_dataframe(df):\n",
    "    # 获取行数和列数\n",
    "    num_rows, num_cols = df.shape\n",
    "    array_length = 1024  # 每个单元格数组的预期长度\n",
    "    \n",
    "    # 初始化重塑后的数据数组\n",
    "    reshaped_data = np.zeros((num_rows, num_cols * array_length))\n",
    "    \n",
    "    # 遍历每一行\n",
    "    for i in range(num_rows):\n",
    "        row_arrays = []\n",
    "        for j in range(num_cols):\n",
    "            # 提取单元格值\n",
    "            cell = df.iloc[i, j]\n",
    "            # 如果不是 NumPy 数组，尝试转换为 NumPy 数组\n",
    "            if not isinstance(cell, np.ndarray):\n",
    "                try:\n",
    "                    cell = np.array(cell, dtype=float)\n",
    "                except Exception as e:\n",
    "                    raise ValueError(f\"无法将 [{i}, {j}] 处的单元格转换为 NumPy 数组：{e}\")\n",
    "            \n",
    "            # 检查数组是否为一维且长度正确\n",
    "            if cell.ndim == 0:\n",
    "                raise ValueError(f\"[{i}, {j}] 处的单元格是零维数组（标量）。预期为一维长度为 {array_length} 的数组。\")\n",
    "            if cell.shape[0] != array_length:\n",
    "                raise ValueError(f\"[{i}, {j}] 处的单元格长度为 {cell.shape[0]}。预期长度为 {array_length}。\")\n",
    "            \n",
    "            row_arrays.append(cell)\n",
    "        \n",
    "        # 拼接该行的所有数组\n",
    "        try:\n",
    "            row_data = np.concatenate(row_arrays)\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"第 {i} 行数组拼接失败：{e}\")\n",
    "        \n",
    "        # 确保拼接后的行长度正确\n",
    "        if row_data.shape[0] != num_cols * array_length:\n",
    "            raise ValueError(f\"第 {i} 行拼接后的数据长度为 {row_data.shape[0]}。预期为 {num_cols * array_length}。\")\n",
    "        \n",
    "        reshaped_data[i, :] = row_data\n",
    "    \n",
    "    # 创建新的 DataFrame\n",
    "    reshaped_df = pd.DataFrame(reshaped_data)\n",
    "    \n",
    "    return reshaped_df\n",
    "\n",
    "# 应用重塑函数\n",
    "try:\n",
    "    reshaped_A = reshape_dataframe(embedding_df)\n",
    "    print(reshaped_A.shape)  # 应输出 (100, 7168)，因为 7 * 1024 = 7168\n",
    "except Exception as e:\n",
    "    print(f\"重塑过程中出错：{e}\")\n",
    "\n",
    "# 可选：根据股票代码分配列名\n",
    "tickers = embedding_df.columns\n",
    "new_columns = [f\"{ticker}_{i}\" for ticker in tickers for i in range(1024)]\n",
    "reshaped_A.columns = new_columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepfin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
