{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7558c5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepdow.data import InRAMDataset, RigidDataLoader\n",
    "from deepdow.layers import SoftmaxAllocator\n",
    "from deepdow.losses import MeanReturns, SharpeRatio, MaximumDrawdown\n",
    "#from deepdow.layers import ConvNetwork\n",
    "from deepdow.experiments import Run\n",
    "from deepdow.losses import MeanReturns, SharpeRatio, MaximumDrawdown\n",
    "from deepdow.benchmarks import OneOverN, InverseVolatility, Random, MinimumVariance\n",
    "from deepdow.callbacks import EarlyStoppingCallback\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from deepdow.utils import raw_to_Xy\n",
    "from deepdow.benchmarks import Benchmark, OneOverN, Random, MaximumReturn\n",
    "from deepdow.experiments import Run\n",
    "from deepdow.callbacks import EarlyStoppingCallback, ModelCheckpointCallback, Callback\n",
    "from deepdow.visualize import generate_metrics_table, generate_weights_table, plot_metrics, plot_weight_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b25838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_dict = {\n",
    "    'task_name': 'long_term_forecast',\n",
    "    'features': 'M',  # or 'S' for single variable\n",
    "    'seq_len': 96,\n",
    "    'label_len': 24,\n",
    "    'pred_len': 24,\n",
    "    'use_norm': True,\n",
    "    'patch_len': 12,\n",
    "    'enc_in': 7,  # number of input features\n",
    "    'd_model': 64,  # model dimension\n",
    "    'embed': 'fixed',  # embedding type\n",
    "    'freq': 'd',  # frequency of the data\n",
    "    'dropout': 0.1,\n",
    "    'factor': 5,  # attention factor\n",
    "    'n_heads': 8,  # number of attention heads\n",
    "    'd_ff': 256,  # feed-forward dimension\n",
    "    'e_layers': 3,  # number of encoder layers\n",
    "    'activation': 'relu',  # activation function\n",
    "    'augmentation_ratio': 0,  # augmentation ratio\n",
    "    'seed': 42,  #\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e05b12b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把configs转换为一个类\n",
    "class Configs:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06fe7ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = Configs(**configs_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8640a76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_provider.data_loader import Dataset_Custom, custom_collate_fn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "train_dataset = Dataset_Custom(\n",
    "    args=configs,\n",
    "    root_path='.',\n",
    "    flag='train',\n",
    "    size=[configs.seq_len, configs.label_len, configs.pred_len],\n",
    "    features=configs.features,\n",
    "    data_path='mag7_ticker_first.parquet',\n",
    "    target='Close',\n",
    "    scale=True,\n",
    "    timeenc=1,\n",
    "    freq='D'\n",
    ")\n",
    "test_dataset = Dataset_Custom(\n",
    "    args=configs,\n",
    "    root_path='.',\n",
    "    flag='test',\n",
    "    size=[configs.seq_len, configs.label_len, configs.pred_len],\n",
    "    features=configs.features,\n",
    "    data_path='mag7_ticker_first.parquet',\n",
    "    target='Close',\n",
    "    scale=True,\n",
    "    timeenc=1,\n",
    "    freq='D'\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=custom_collate_fn, batch_size=32, shuffle=True)\n",
    "val_dataloader = DataLoader(test_dataset, collate_fn=custom_collate_fn, batch_size=32, shuffle=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee83ec37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: torch.Size([32, 5, 96, 7])\n",
      "y shape: torch.Size([32, 5, 48, 7])\n",
      "Timestamps: [numpy.datetime64('2024-12-20T00:00:00.000000000'), numpy.datetime64('2021-12-31T00:00:00.000000000'), numpy.datetime64('2021-07-21T00:00:00.000000000'), numpy.datetime64('2022-03-09T00:00:00.000000000'), numpy.datetime64('2023-12-05T00:00:00.000000000'), numpy.datetime64('2021-12-14T00:00:00.000000000'), numpy.datetime64('2024-02-01T00:00:00.000000000'), numpy.datetime64('2021-10-06T00:00:00.000000000'), numpy.datetime64('2024-05-02T00:00:00.000000000'), numpy.datetime64('2024-05-31T00:00:00.000000000'), numpy.datetime64('2024-12-26T00:00:00.000000000'), numpy.datetime64('2021-12-27T00:00:00.000000000'), numpy.datetime64('2021-06-28T00:00:00.000000000'), numpy.datetime64('2024-09-19T00:00:00.000000000'), numpy.datetime64('2024-08-21T00:00:00.000000000'), numpy.datetime64('2023-06-22T00:00:00.000000000'), numpy.datetime64('2022-03-08T00:00:00.000000000'), numpy.datetime64('2022-03-07T00:00:00.000000000'), numpy.datetime64('2024-02-23T00:00:00.000000000'), numpy.datetime64('2022-12-08T00:00:00.000000000'), numpy.datetime64('2021-09-30T00:00:00.000000000'), numpy.datetime64('2023-12-21T00:00:00.000000000'), numpy.datetime64('2023-08-30T00:00:00.000000000'), numpy.datetime64('2025-01-31T00:00:00.000000000'), numpy.datetime64('2021-06-23T00:00:00.000000000'), numpy.datetime64('2022-12-09T00:00:00.000000000'), numpy.datetime64('2021-07-08T00:00:00.000000000'), numpy.datetime64('2021-09-13T00:00:00.000000000'), numpy.datetime64('2022-09-16T00:00:00.000000000'), numpy.datetime64('2021-08-17T00:00:00.000000000'), numpy.datetime64('2024-01-18T00:00:00.000000000'), numpy.datetime64('2022-05-17T00:00:00.000000000')]\n",
      "kw shape: [(array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object), array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object), array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object), array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object), array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object), array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object), array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object), array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object), array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object), array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object), array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object), array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object), array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object), array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object), array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object), array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object), array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object), array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object), array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object), array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object), array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object), array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object), array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object), array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object), array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object), array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object), array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object), array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object), array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object), array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object), array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object), array(['AAPL', 'AMZN', 'GOOGL', 'META', 'MSFT', 'NVDA', 'TSLA'],\n",
      "      dtype=object)), tensor([[[-0.3333, -0.3333,  0.0973],\n",
      "         [-0.1667, -0.3000,  0.1000],\n",
      "         [ 0.0000, -0.2667,  0.1027],\n",
      "         ...,\n",
      "         [-0.3333,  0.0333,  0.4616],\n",
      "         [-0.1667,  0.0667,  0.4644],\n",
      "         [ 0.0000,  0.1000,  0.4671]],\n",
      "\n",
      "        [[-0.5000,  0.0000,  0.1219],\n",
      "         [-0.3333,  0.0333,  0.1247],\n",
      "         [-0.1667,  0.0667,  0.1274],\n",
      "         ...,\n",
      "         [-0.3333,  0.4000,  0.4890],\n",
      "         [-0.1667,  0.4333,  0.4918],\n",
      "         [ 0.0000,  0.4667,  0.4945]],\n",
      "\n",
      "        [[ 0.0000, -0.4000, -0.3301],\n",
      "         [ 0.1667, -0.3667, -0.3274],\n",
      "         [-0.5000, -0.2667, -0.3192],\n",
      "         ...,\n",
      "         [ 0.1667,  0.0000,  0.0370],\n",
      "         [-0.5000,  0.1000,  0.0452],\n",
      "         [-0.3333,  0.1333,  0.0479]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1667,  0.5000, -0.2562],\n",
      "         [ 0.0000, -0.5000, -0.2534],\n",
      "         [-0.5000, -0.3667, -0.2425],\n",
      "         ...,\n",
      "         [ 0.0000, -0.1333,  0.1110],\n",
      "         [ 0.1667, -0.1000,  0.1137],\n",
      "         [-0.5000,  0.0000,  0.1219]],\n",
      "\n",
      "        [[-0.1667,  0.4667,  0.1603],\n",
      "         [ 0.0000,  0.5000,  0.1630],\n",
      "         [ 0.1667, -0.5000,  0.1658],\n",
      "         ...,\n",
      "         [ 0.1667, -0.1333, -0.4699],\n",
      "         [-0.3333,  0.0000, -0.4589],\n",
      "         [-0.1667,  0.0333, -0.4562]],\n",
      "\n",
      "        [[-0.1667,  0.4333,  0.4918],\n",
      "         [ 0.0000,  0.4667,  0.4945],\n",
      "         [ 0.1667,  0.5000,  0.4973],\n",
      "         ...,\n",
      "         [ 0.0000, -0.1333, -0.1411],\n",
      "         [ 0.1667, -0.1000, -0.1384],\n",
      "         [-0.5000,  0.0000, -0.1301]]], dtype=torch.float64), tensor([[[ 0.1667, -0.0333,  0.3740],\n",
      "         [-0.5000,  0.0667,  0.3822],\n",
      "         [-0.3333,  0.1000,  0.3849],\n",
      "         ...,\n",
      "         [ 0.1667,  0.2667, -0.4370],\n",
      "         [-0.5000,  0.3667, -0.4288],\n",
      "         [-0.3333,  0.4000, -0.4260]],\n",
      "\n",
      "        [[ 0.1667,  0.3333,  0.4014],\n",
      "         [-0.5000,  0.4333,  0.4096],\n",
      "         [-0.3333,  0.4667,  0.4123],\n",
      "         ...,\n",
      "         [-0.3333, -0.5000, -0.4151],\n",
      "         [-0.1667, -0.4667, -0.4123],\n",
      "         [ 0.0000, -0.4333, -0.4096]],\n",
      "\n",
      "        [[-0.1667,  0.0000, -0.0452],\n",
      "         [ 0.0000,  0.0333, -0.0425],\n",
      "         [ 0.1667,  0.0667, -0.0397],\n",
      "         ...,\n",
      "         [ 0.0000,  0.1000,  0.1301],\n",
      "         [ 0.1667,  0.1333,  0.1329],\n",
      "         [-0.5000,  0.2333,  0.1411]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1667, -0.0667,  0.0315],\n",
      "         [ 0.0000, -0.0333,  0.0342],\n",
      "         [ 0.1667,  0.0000,  0.0370],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.2068],\n",
      "         [ 0.1667,  0.0333,  0.2096],\n",
      "         [-0.5000,  0.1333,  0.2178]],\n",
      "\n",
      "        [[-0.3333, -0.1333,  0.4452],\n",
      "         [-0.1667, -0.1000,  0.4479],\n",
      "         [ 0.0000, -0.0667,  0.4507],\n",
      "         ...,\n",
      "         [ 0.1667,  0.0000, -0.3740],\n",
      "         [-0.3333,  0.1333, -0.3630],\n",
      "         [-0.1667,  0.1667, -0.3603]],\n",
      "\n",
      "        [[-0.3333, -0.1333, -0.2233],\n",
      "         [-0.1667, -0.1000, -0.2205],\n",
      "         [ 0.0000, -0.0667, -0.2178],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000, -0.0452],\n",
      "         [ 0.1667,  0.0333, -0.0425],\n",
      "         [-0.3333,  0.1667, -0.0315]]], dtype=torch.float64)]\n"
     ]
    }
   ],
   "source": [
    "for batch in val_dataloader:\n",
    "    X_batch, y_batch, timestamps_batch, *kw = batch\n",
    "    print(\"X shape:\", X_batch.shape)\n",
    "    print(\"y shape:\", y_batch.shape)\n",
    "    print(\"Timestamps:\", timestamps_batch)\n",
    "    print(\"kw shape:\", [k for k in kw])\n",
    "    break  # 只打印第一个批次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb676b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from layers.SelfAttention_Family import FullAttention, AttentionLayer\n",
    "from layers.Embed import DataEmbedding_inverted, PositionalEmbedding\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class FlattenHead(nn.Module):\n",
    "    def __init__(self, n_vars, nf, target_window, head_dropout=0):\n",
    "        super().__init__()\n",
    "        self.n_vars = n_vars\n",
    "        self.flatten = nn.Flatten(start_dim=-2)\n",
    "        self.linear = nn.Linear(nf, target_window)\n",
    "        self.dropout = nn.Dropout(head_dropout)\n",
    "\n",
    "    def forward(self, x):  # x: [bs x nvars x d_model x patch_num]\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EnEmbedding(nn.Module):\n",
    "    def __init__(self, n_vars, d_model, patch_len, dropout):\n",
    "        super(EnEmbedding, self).__init__()\n",
    "        # Patching\n",
    "        self.patch_len = patch_len\n",
    "\n",
    "        self.value_embedding = nn.Linear(patch_len, d_model, bias=False)\n",
    "        self.glb_token = nn.Parameter(torch.randn(1, n_vars, 1, d_model))\n",
    "        self.position_embedding = PositionalEmbedding(d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # do patching\n",
    "        n_vars = x.shape[1]\n",
    "        glb = self.glb_token.repeat((x.shape[0], 1, 1, 1))\n",
    "\n",
    "        x = x.unfold(dimension=-1, size=self.patch_len, step=self.patch_len)\n",
    "        x = torch.reshape(x, (x.shape[0] * x.shape[1], x.shape[2], x.shape[3]))\n",
    "        # Input encoding\n",
    "        x = self.value_embedding(x) + self.position_embedding(x)\n",
    "        x = torch.reshape(x, (-1, n_vars, x.shape[-2], x.shape[-1]))\n",
    "        x = torch.cat([x, glb], dim=2)\n",
    "        x = torch.reshape(x, (x.shape[0] * x.shape[1], x.shape[2], x.shape[3]))\n",
    "        return self.dropout(x), n_vars\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layers, norm_layer=None, projection=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.norm = norm_layer\n",
    "        self.projection = projection\n",
    "\n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None, tau=None, delta=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask, tau=tau, delta=delta)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        if self.projection is not None:\n",
    "            x = self.projection(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, self_attention, cross_attention, d_model, d_ff=None,\n",
    "                 dropout=0.1, activation=\"relu\"):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.self_attention = self_attention\n",
    "        self.cross_attention = cross_attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None, tau=None, delta=None):\n",
    "        B, L, D = cross.shape\n",
    "        x = x + self.dropout(self.self_attention(\n",
    "            x, x, x,\n",
    "            attn_mask=x_mask,\n",
    "            tau=tau, delta=None\n",
    "        )[0])\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        x_glb_ori = x[:, -1, :].unsqueeze(1)\n",
    "        x_glb = torch.reshape(x_glb_ori, (B, -1, D))\n",
    "        x_glb_attn = self.dropout(self.cross_attention(\n",
    "            x_glb, cross, cross,\n",
    "            attn_mask=cross_mask,\n",
    "            tau=tau, delta=delta\n",
    "        )[0])\n",
    "        x_glb_attn = torch.reshape(x_glb_attn,\n",
    "                                   (x_glb_attn.shape[0] * x_glb_attn.shape[1], x_glb_attn.shape[2])).unsqueeze(1)\n",
    "        x_glb = x_glb_ori + x_glb_attn\n",
    "        x_glb = self.norm2(x_glb)\n",
    "\n",
    "        y = x = torch.cat([x[:, :-1, :], x_glb], dim=1)\n",
    "\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "\n",
    "        return self.norm3(x + y)\n",
    "\n",
    "\n",
    "class Model(nn.Module, Benchmark):\n",
    "\n",
    "    def __init__(self, configs):\n",
    "        super(Model, self).__init__()\n",
    "        self.task_name = configs.task_name\n",
    "        self.features = configs.features\n",
    "        self.seq_len = configs.seq_len\n",
    "        self.pred_len = configs.pred_len\n",
    "        self.use_norm = configs.use_norm\n",
    "        self.patch_len = configs.patch_len\n",
    "        self.patch_num = int(configs.seq_len // configs.patch_len)\n",
    "        self.n_vars = 1 if configs.features == 'MS' else configs.enc_in\n",
    "        # Embedding\n",
    "        self.en_embedding = EnEmbedding(self.n_vars, configs.d_model, self.patch_len, configs.dropout)\n",
    "\n",
    "        self.ex_embedding = DataEmbedding_inverted(configs.seq_len, configs.d_model, configs.embed, configs.freq,\n",
    "                                                   configs.dropout)\n",
    "\n",
    "        # Encoder-only architecture\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(False, configs.factor, attention_dropout=configs.dropout,\n",
    "                                      output_attention=False),\n",
    "                        configs.d_model, configs.n_heads),\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(False, configs.factor, attention_dropout=configs.dropout,\n",
    "                                      output_attention=False),\n",
    "                        configs.d_model, configs.n_heads),\n",
    "                    configs.d_model,\n",
    "                    configs.d_ff,\n",
    "                    dropout=configs.dropout,\n",
    "                    activation=configs.activation,\n",
    "                )\n",
    "                for l in range(configs.e_layers)\n",
    "            ],\n",
    "            norm_layer=torch.nn.LayerNorm(configs.d_model)\n",
    "        )\n",
    "        self.head_nf = configs.d_model * (self.patch_num + 1)\n",
    "        self.head = FlattenHead(configs.enc_in, self.head_nf, configs.pred_len,\n",
    "                                head_dropout=configs.dropout)\n",
    "\n",
    "    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "        if self.use_norm:\n",
    "            # Normalization from Non-stationary Transformer\n",
    "            means = x_enc.mean(1, keepdim=True).detach()\n",
    "            x_enc = x_enc - means\n",
    "            stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "            x_enc /= stdev\n",
    "\n",
    "        _, _, N = x_enc.shape\n",
    "\n",
    "        en_embed, n_vars = self.en_embedding(x_enc[:, :, -1].unsqueeze(-1).permute(0, 2, 1))\n",
    "        ex_embed = self.ex_embedding(x_enc[:, :, :-1], x_mark_enc)\n",
    "\n",
    "        enc_out = self.encoder(en_embed, ex_embed)\n",
    "        enc_out = torch.reshape(\n",
    "            enc_out, (-1, n_vars, enc_out.shape[-2], enc_out.shape[-1]))\n",
    "        # z: [bs x nvars x d_model x patch_num]\n",
    "        enc_out = enc_out.permute(0, 1, 3, 2)\n",
    "\n",
    "        dec_out = self.head(enc_out)  # z: [bs x nvars x target_window]\n",
    "        dec_out = dec_out.permute(0, 2, 1)\n",
    "\n",
    "        if self.use_norm:\n",
    "            # De-Normalization from Non-stationary Transformer\n",
    "            dec_out = dec_out * (stdev[:, 0, -1:].unsqueeze(1).repeat(1, self.pred_len, 1))\n",
    "            dec_out = dec_out + (means[:, 0, -1:].unsqueeze(1).repeat(1, self.pred_len, 1))\n",
    "\n",
    "        return dec_out\n",
    "\n",
    "\n",
    "    def forecast_multi(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "        if self.use_norm:\n",
    "            # Normalization from Non-stationary Transformer\n",
    "            means = x_enc.mean(1, keepdim=True).detach()\n",
    "            x_enc = x_enc - means\n",
    "            stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "            x_enc /= stdev\n",
    "\n",
    "        _, _, N = x_enc.shape\n",
    "\n",
    "        en_embed, n_vars = self.en_embedding(x_enc.permute(0, 2, 1))\n",
    "        ex_embed = self.ex_embedding(x_enc, x_mark_enc)\n",
    "\n",
    "        enc_out = self.encoder(en_embed, ex_embed)\n",
    "        enc_out = torch.reshape(\n",
    "            enc_out, (-1, n_vars, enc_out.shape[-2], enc_out.shape[-1]))\n",
    "        # z: [bs x nvars x d_model x patch_num]\n",
    "        enc_out = enc_out.permute(0, 1, 3, 2)\n",
    "\n",
    "        dec_out = self.head(enc_out)  # z: [bs x nvars x target_window]\n",
    "        dec_out = dec_out.permute(0, 2, 1)\n",
    "\n",
    "        if self.use_norm:\n",
    "            # De-Normalization from Non-stationary Transformer\n",
    "            dec_out = dec_out * (stdev[:, 0, :].unsqueeze(1).repeat(1, self.pred_len, 1))\n",
    "            dec_out = dec_out + (means[:, 0, :].unsqueeze(1).repeat(1, self.pred_len, 1))\n",
    "\n",
    "        return dec_out\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n",
    "        if self.task_name == 'long_term_forecast' or self.task_name == 'short_term_forecast':\n",
    "            if self.features == 'M':\n",
    "                dec_out = self.forecast_multi(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
    "                return dec_out[:, -self.pred_len:, :]  # [B, L, D]\n",
    "            else:\n",
    "                dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
    "                return dec_out[:, -self.pred_len:, :]  # [B, L, D]\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6ba9712",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Model(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b17008e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = MaximumDrawdown(input_type='simple') + MeanReturns(input_type='simple') + 10 * SharpeRatio(input_type='simple', rf=0.000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46fdab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from deepdow.experiments import Run, SolverError, EarlyStoppingException\n",
    "class ExtendedRun(Run):\n",
    "    def launch(self, n_epochs=1):\n",
    "        try:\n",
    "            self.network.to(device=self.device, dtype=self.dtype)\n",
    "            if self.current_epoch == -1:\n",
    "                self.on_train_begin(metadata={\"n_epochs\": n_epochs})\n",
    "\n",
    "            for _ in range(n_epochs):\n",
    "                self.current_epoch += 1\n",
    "                self.on_epoch_begin(metadata={\"epoch\": self.current_epoch})\n",
    "\n",
    "                for batch_ix, batch in enumerate(self.train_dataloader):\n",
    "                    # 解包以支持额外变量\n",
    "                    X_batch, y_batch, timestamps, asset_names, *extra = batch\n",
    "                    extra_data = extra[0] if extra else None  # 处理无额外数据的情况\n",
    "\n",
    "                    self.on_batch_begin(\n",
    "                        metadata={\n",
    "                            \"asset_names\": asset_names,\n",
    "                            \"batch\": batch_ix,\n",
    "                            \"epoch\": self.current_epoch,\n",
    "                            \"timestamps\": timestamps,\n",
    "                            \"X_batch\": X_batch,\n",
    "                            \"y_batch\": y_batch,\n",
    "                            \"extra_data\": extra_data,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    X_batch, y_batch = X_batch.to(self.device).to(self.dtype), y_batch.to(self.device).to(self.dtype)\n",
    "                    if extra_data is not None:\n",
    "                        extra_data = extra_data.to(self.device).to(self.dtype)\n",
    "\n",
    "                    self.network.train()\n",
    "                    \n",
    "                    X_mark = extra[0]\n",
    "                    y_mark = extra[1] if len(extra) > 1 else None\n",
    "                    weights = self.network(X_batch, y_batch, X_mark, y_mark)\n",
    "                    # 如果支持，将 extra_data 传递给损失函数\n",
    "                    loss_per_sample = self.loss(weights, y_batch, extra_data=extra_data)\n",
    "                    loss = loss_per_sample.mean()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    self.network.eval()\n",
    "\n",
    "                    self.on_batch_end(\n",
    "                        metadata={\n",
    "                            \"asset_names\": asset_names,\n",
    "                            \"batch\": batch_ix,\n",
    "                            \"batch_loss\": loss.item(),\n",
    "                            \"epoch\": self.current_epoch,\n",
    "                            \"timestamps\": timestamps,\n",
    "                            \"weights\": weights,\n",
    "                            \"X_batch\": X_batch,\n",
    "                            \"y_batch\": y_batch,\n",
    "                            \"extra_data\": extra_data,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                self.on_epoch_end(\n",
    "                    metadata={\n",
    "                        \"epoch\": self.current_epoch,\n",
    "                        \"n_epochs\": n_epochs,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            self.on_train_end()\n",
    "\n",
    "        except (EarlyStoppingException, KeyboardInterrupt, SolverError) as ex:\n",
    "            print(\"训练中断\")\n",
    "            time.sleep(1)\n",
    "            self.on_train_interrupt(metadata={\"exception\": ex, \"locals\": locals()})\n",
    "\n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7686fd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "d_model = configs.d_model\n",
    "n_heads = configs.n_heads\n",
    "n_layers = configs.e_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3723592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#获得时间 作为文件名\n",
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "date_time = now.strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "run = ExtendedRun(network,\n",
    "          loss,\n",
    "          train_dataloader,\n",
    "          benchmarks={'OneOverN': OneOverN(),\n",
    "                      'MaximumReturn': MaximumReturn(),\n",
    "                     },\n",
    "          val_dataloaders={'test': val_dataloader},\n",
    "          optimizer=torch.optim.Adam(network.parameters(), amsgrad=True, lr=0.001),\n",
    "          callbacks=[EarlyStoppingCallback(metric_name='loss',\n",
    "                                           dataloader_name='test',\n",
    "                                           patience=15),\n",
    "                     ModelCheckpointCallback(folder_path=f'./models/{date_time}_d_model_{d_model}_nhead{n_heads}_num_layers{n_layers}/',\n",
    "                                             dataloader_name='test',\n",
    "                                             metric_name='loss')],\n",
    "          device=device,\n",
    "          #dtype=torch.float64,\n",
    "          )\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d970f7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = run.launch(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepfin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
