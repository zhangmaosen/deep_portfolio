{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output (memory) shape: torch.Size([2, 10, 512])\n",
      "Decoder output shape: torch.Size([2, 8, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 参数\n",
    "d_model = 512  # 隐藏维度\n",
    "nhead = 8      # 注意力头数\n",
    "num_encoder_layers = 6  # 编码器层数\n",
    "num_decoder_layers = 6  # 解码器层数\n",
    "dim_feedforward = 2048  # 前馈层中间维度\n",
    "dropout = 0.1\n",
    "\n",
    "# 定义编码器和解码器层\n",
    "encoder_layer = nn.TransformerEncoderLayer(\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    dropout=dropout,\n",
    "    batch_first=True\n",
    ")\n",
    "decoder_layer = nn.TransformerDecoderLayer(\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    dropout=dropout,\n",
    "    batch_first=True\n",
    ")\n",
    "\n",
    "# 堆叠多层\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
    "\n",
    "# 输入数据\n",
    "batch_size = 2\n",
    "src_seq_len = 10  # 源序列长度\n",
    "tgt_seq_len = 8   # 目标序列长度\n",
    "src = torch.randn(batch_size, src_seq_len, d_model)  # 源序列\n",
    "tgt = torch.randn(batch_size, tgt_seq_len, d_model)  # 目标序列\n",
    "\n",
    "# 生成掩码（自回归掩码）\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = torch.triu(torch.ones(sz, sz), diagonal=1).bool()\n",
    "    return mask\n",
    "\n",
    "tgt_mask = generate_square_subsequent_mask(tgt_seq_len).to(src.device)\n",
    "\n",
    "# 前向传播\n",
    "# (1) 编码器处理源序列\n",
    "memory = transformer_encoder(src)  # 输出形状: (batch_size, src_seq_len, d_model)\n",
    "\n",
    "# (2) 解码器处理目标序列，并使用编码器输出进行交叉注意力\n",
    "output = transformer_decoder(\n",
    "    tgt,              # 目标序列\n",
    "    memory,           # 编码器输出\n",
    "    tgt_mask=tgt_mask # 自回归掩码\n",
    ")  # 输出形状: (batch_size, tgt_seq_len, d_model)\n",
    "\n",
    "# 打印结果\n",
    "print(\"Encoder output (memory) shape:\", memory.shape)\n",
    "print(\"Decoder output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
